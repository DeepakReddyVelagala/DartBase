{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-27T06:44:59.960906Z",
     "iopub.status.busy": "2023-05-27T06:44:59.960365Z",
     "iopub.status.idle": "2023-05-27T06:45:03.211968Z",
     "shell.execute_reply": "2023-05-27T06:45:03.210935Z",
     "shell.execute_reply.started": "2023-05-27T06:44:59.960865Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torchvision.transforms.functional\n",
    "\n",
    "\n",
    "class DoubleConvolution(nn.Module):\n",
    "  def __init__(self, in_channels, out_channels):\n",
    "    super().__init__()\n",
    "\n",
    "    # first convolution with kerel size = 3 and stride = 1 (padding 1 added so output will have same size as input)\n",
    "    self.first = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "    self.act1 = nn.ReLU()\n",
    "    self.second = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "    self.act2 = nn.ReLU()\n",
    "  \n",
    "  def forward(self, x):\n",
    "    x = self.first(x);\n",
    "    x = self.act1(x);\n",
    "    x = self.second(x);\n",
    "    return self.act2(x)\n",
    "\n",
    "\n",
    "class DownSample(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "\n",
    "    self.pool = nn.MaxPool2d(2)\n",
    "\n",
    "  def forward(self, x):\n",
    "    return self.pool(x)\n",
    "\n",
    "\n",
    "class UpSample(nn.Module):\n",
    "  def __init__(self, in_channels, out_channels):\n",
    "    super().__init__()\n",
    "    \n",
    "    self.up = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n",
    "  \n",
    "  def forward(self, x):\n",
    "    return self.up(x)\n",
    "\n",
    "\n",
    "class CropAndConcat(nn.Module):\n",
    "  def forward(self, x, contracting_x):\n",
    "    contracting_x = torchvision.transforms.functional.center_crop(contracting_x, [x.shape[2], x.shape[3]])\n",
    "    x = torch.cat([x, contracting_x], dim=1)\n",
    "    return x\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "  def __init__(self, in_channels, out_channels):\n",
    "    super().__init__()\n",
    "    self.down_conv = nn.ModuleList([DoubleConvolution(i,o) for i, o in [(in_channels, 64), (64, 128), (128, 256), (256, 512)]])\n",
    "    self.down_sample = nn.ModuleList([DownSample() for _ in range(4)])\n",
    "    self.middle_conv = DoubleConvolution(512, 1024)\n",
    "    self.up_sample = nn.ModuleList([UpSample(i, o) for i, o in [(1024, 512), (512, 256), (256, 128), (128, 64)]])\n",
    "    self.up_conv = nn.ModuleList([DoubleConvolution(i, o) for i, o in [(1024, 512), (512, 256), (256, 128), (128, 64)]])\n",
    "    self.concat = nn.ModuleList([CropAndConcat() for _ in range(4)])\n",
    "    self.final_conv = nn.Conv2d(64, out_channels, kernel_size=1)\n",
    "  \n",
    "  def forward(self, x):\n",
    "    pass_through = []\n",
    "    for i in range(len(self.down_conv)):\n",
    "      x = self.down_conv[i](x)\n",
    "      pass_through.append(x)\n",
    "      x = self.down_sample[i](x)\n",
    "    x = self.middle_conv(x)\n",
    "    for i in range(len(self.up_conv)):\n",
    "      x = self.up_sample[i](x)\n",
    "      x = self.concat[i](x, pass_through.pop())\n",
    "      x = self.up_conv[i](x)\n",
    "    x = self.final_conv(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-27T06:45:03.215004Z",
     "iopub.status.busy": "2023-05-27T06:45:03.214238Z",
     "iopub.status.idle": "2023-05-27T06:45:03.389314Z",
     "shell.execute_reply": "2023-05-27T06:45:03.387557Z",
     "shell.execute_reply.started": "2023-05-27T06:45:03.214951Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torchvision.transforms.functional\n",
    "from torch import nn\n",
    "\n",
    "from labml import lab, tracker, experiment, monit\n",
    "from labml.configs import BaseConfigs\n",
    "from labml_helpers.device import DeviceConfigs\n",
    "from labml_nn.unet.carvana import CarvanaDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-27T07:00:03.466165Z",
     "iopub.status.busy": "2023-05-27T07:00:03.465722Z",
     "iopub.status.idle": "2023-05-27T07:00:03.543389Z",
     "shell.execute_reply": "2023-05-27T07:00:03.541675Z",
     "shell.execute_reply.started": "2023-05-27T07:00:03.466132Z"
    }
   },
   "outputs": [],
   "source": [
    "class Configs(BaseConfigs):\n",
    "  device: torch.device = DeviceConfigs()\n",
    "  model: UNet\n",
    "  image_channels = 3\n",
    "  mask_channels = 1\n",
    "  batch_size = 1\n",
    "  learning_rate = 2.5e-4\n",
    "  epochs = 4\n",
    "  dataset: CarvanaDataset\n",
    "  data_loader: torch.utils.data.DataLoader\n",
    "  loss_func = nn.BCELoss()\n",
    "  sigmoid = nn.Sigmoid()\n",
    "  optimizer: torch.optim.Adam\n",
    "\n",
    "  def init(self):\n",
    "    self.dataset = CarvanaDataset(lab.get_data_path()/'carvana'/'train',\n",
    "                                  lab.get_data_path()/'carvana'/'train_masks')\n",
    "    self.model = UNet(self.image_channels, self.mask_channels).to(self.device)\n",
    "    self.data_loader = torch.utils.data.DataLoader(self.dataset, self.batch_size, shuffle=True, pin_memory=True)\n",
    "    self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "    tracker.set_image(\"sample\", True)\n",
    "\n",
    "  @torch.no_grad()\n",
    "  def sample(self, idx=-1):\n",
    "    x, _ = self.dataset[np.random.randint(len(self.dataset))]\n",
    "    x = x.to(self.device)\n",
    "    mask = self.sigmoid(self.model(x[None,:]))\n",
    "    x = torchvision.tranforms.functional.center_crop(x, [mask.shape[2], mask.shape[3]])\n",
    "    tracker.save('sample', x*mask)\n",
    "\n",
    "  def train(self):\n",
    "    for _, (image, mask) in monit.mix(('Train', self.data_loader), (self.sample, list(range(50)))):\n",
    "      tracker.add_global_step()\n",
    "      image, mask = image.to(self.device), mask.to(self.device)\n",
    "      self.optimizer.zero_grad()\n",
    "      logits = self.model(image)\n",
    "\n",
    "      mask=torchvision.transforms.functional.center_crop(mask, [logits.shape[2], logits.shape[3]])\n",
    "      loss = self.loss_func(self.sigmoid(logits), mask)\n",
    "      loss.backward()\n",
    "      self.optimizer.step()\n",
    "      tracker.save('loss', loss)\n",
    "\n",
    "  def run(self):\n",
    "    for _ in monit.loop(self.epochs):\n",
    "      self.train()\n",
    "      tracker.new_line()\n",
    "      experiment.save_checkpoint()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-05-27T06:45:03.393756Z",
     "iopub.status.idle": "2023-05-27T06:45:03.395503Z",
     "shell.execute_reply": "2023-05-27T06:45:03.395253Z",
     "shell.execute_reply.started": "2023-05-27T06:45:03.395231Z"
    }
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "  experiment.create(name='unet')\n",
    "  configs = Configs()\n",
    "  experiment.configs(configs, {})\n",
    "  configs.init()\n",
    "  experiment.add_pytorch_models({'model': configs.model})\n",
    "  with experiment.start():\n",
    "    configs.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-27T06:45:04.294668Z",
     "iopub.status.busy": "2023-05-27T06:45:04.293726Z",
     "iopub.status.idle": "2023-05-27T06:45:04.330723Z",
     "shell.execute_reply": "2023-05-27T06:45:04.329414Z",
     "shell.execute_reply.started": "2023-05-27T06:45:04.294624Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"overflow-x: scroll;\">\n",
       "\n",
       "<strong><span style=\"text-decoration: underline\">unet</span></strong>: <span style=\"color: #208FFB\">20c33fc2ff6211eda1d76c6a779c7804</span>\n",
       "\t[dirty]: <strong><span style=\"color: #DDB62B\">\"Initial commit\"</span></strong>\n",
       "<strong><span style=\"color: #DDB62B\">Still updating app.labml.ai, please wait for it to complete...</span></strong></pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 232.00 MiB (GPU 0; 3.81 GiB total capacity; 2.57 GiB already allocated; 70.38 MiB free; 2.85 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[26], line 8\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m experiment\u001b[38;5;241m.\u001b[39madd_pytorch_models({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m: configs\u001b[38;5;241m.\u001b[39mmodel})\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m experiment\u001b[38;5;241m.\u001b[39mstart():\n\u001b[0;32m----> 8\u001b[0m   \u001b[43mconfigs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[25], line 46\u001b[0m, in \u001b[0;36mConfigs.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     45\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m monit\u001b[38;5;241m.\u001b[39mloop(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepochs):\n\u001b[0;32m---> 46\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m     tracker\u001b[38;5;241m.\u001b[39mnew_line()\n\u001b[1;32m     48\u001b[0m     experiment\u001b[38;5;241m.\u001b[39msave_checkpoint()\n",
      "Cell \u001b[0;32mIn[25], line 36\u001b[0m, in \u001b[0;36mConfigs.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     34\u001b[0m image, mask \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice), mask\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 36\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m mask\u001b[38;5;241m=\u001b[39mtorchvision\u001b[38;5;241m.\u001b[39mtransforms\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mcenter_crop(mask, [logits\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m], logits\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m3\u001b[39m]])\n\u001b[1;32m     39\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_func(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msigmoid(logits), mask)\n",
      "File \u001b[0;32m~/anaconda3/envs/ml_base/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[1], line 70\u001b[0m, in \u001b[0;36mUNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup_conv)):\n\u001b[1;32m     69\u001b[0m   x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup_sample[i](x)\n\u001b[0;32m---> 70\u001b[0m   x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpass_through\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m   x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup_conv[i](x)\n\u001b[1;32m     72\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_conv(x)\n",
      "File \u001b[0;32m~/anaconda3/envs/ml_base/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[1], line 46\u001b[0m, in \u001b[0;36mCropAndConcat.forward\u001b[0;34m(self, x, contracting_x)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, contracting_x):\n\u001b[1;32m     45\u001b[0m   contracting_x \u001b[38;5;241m=\u001b[39m torchvision\u001b[38;5;241m.\u001b[39mtransforms\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mcenter_crop(contracting_x, [x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m], x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m3\u001b[39m]])\n\u001b[0;32m---> 46\u001b[0m   x \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontracting_x\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 232.00 MiB (GPU 0; 3.81 GiB total capacity; 2.57 GiB already allocated; 70.38 MiB free; 2.85 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']=''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
